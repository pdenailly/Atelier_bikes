{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMXEYZ+VOSP5PnXIfbrMQee",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pdenailly/Atelier_bikes/blob/main/ACTIVITE1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANALYSE DE BASES DE DONNEES A L'AIDE DE STATISTIQUES SIMPLES**\n",
        "\n",
        "Application : Etude des volumes de cyclistes dans la ville de Paris en amont, pendant et après les JO 2024.\n"
      ],
      "metadata": {
        "id": "4Sr4PokkhgV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bienvenu dans cet atelier ! Nous allons y explorer quelques manières simples d'étudier un jeux de données avant toute utilisation pour des travaux de modélisation.\n",
        "\n",
        "Pour ce cas pratique nous nous intéresserons à des séries temporelles de traffic de vélo, comptés à chaque heure entre juin et septembre 2024 en plusieurs points de la ville de Paris. Ces données ont été extraites d'une [base ouverte mise à disposition par la ville de Paris](https://parisdata.opendatasoft.com/explore/dataset/comptage-velo-donnees-compteurs/api/?disjunctive.id_compteur&disjunctive.nom_compteur&disjunctive.id&disjunctive.name&q.timerange.date=date:%5B2024-05-31T22:00:00Z+TO+2024-09-13T21:59:59Z%5D&basemap=jawg.dark&location=13,48.85652,2.35024&dataChart=eyJxdWVyaWVzIjpbeyJjaGFydHMiOlt7InR5cGUiOiJjb2x1bW4iLCJmdW5jIjoiQVZHIiwieUF4aXMiOiJzdW1fY291bnRzIiwic2NpZW50aWZpY0Rpc3BsYXkiOnRydWUsImNvbG9yIjoiI0ZBOEM0NCJ9XSwieEF4aXMiOiJkYXRlIiwibWF4cG9pbnRzIjoiIiwidGltZXNjYWxlIjoibW9udGgiLCJzb3J0IjoiIiwiY29uZmlnIjp7ImRhdGFzZXQiOiJjb21wdGFnZS12ZWxvLWRvbm5lZXMtY29tcHRldXJzIiwib3B0aW9ucyI6eyJkaXNqdW5jdGl2ZS5pZF9jb21wdGV1ciI6dHJ1ZSwiZGlzanVuY3RpdmUubm9tX2NvbXB0ZXVyIjp0cnVlLCJkaXNqdW5jdGl2ZS5pZCI6dHJ1ZSwiZGlzanVuY3RpdmUubmFtZSI6dHJ1ZSwicmVmaW5lLmRhdGUiOiIyMDI0In19fV0sImRpc3BsYXlMZWdlbmQiOnRydWUsImFsaWduTW9udGgiOnRydWUsInRpbWVzY2FsZSI6IiJ9). La période de Juillet 2024 a été marquée par la présence des Jeux Olympiques en région parisienne, l'un des mots d'ordres ayant été l'utilisation des mobilités douces, dont le vélo. A travers les différents ateliers proposés ici, nous allons nous intéresser à l'étude de ces données, et observer à quel point les jeux olympiques ont eu un impact sur le volume de cyclistes à Paris.\n",
        "\n",
        "Assurez vous que le fichier de données *comptage-velo-donnees-compteurs.csv* soit présent dans le répertoire courant (sur colab, faites les glisser dans l'espace de stockage de la session avec le logo dossier à gauche). Ces fichiers sont disponibles sur le dépôt git de l'atelier : https://github.com/pdenailly/Atelier-Summer-School."
      ],
      "metadata": {
        "id": "yiLBV48thtFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Importation des librairies Python**\n",
        "\n",
        "Nous allons travailler avec le language Python dans cet atelier. En amont de tout projet, il est toujours bon de rassembler les importations de librairies dans une section dédiée. Nous allons notamment travailler avec la librairie *pandas*, très adaptée à l'étude de bases de données sous python."
      ],
      "metadata": {
        "id": "7IiQ_nDmsc0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "J2IZ61MLs429"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importation des données de comptages de vélos**"
      ],
      "metadata": {
        "id": "U_1WaBsHtR88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'importation des données se fait avec la librairie pandas, via la fonction read_csv(.)."
      ],
      "metadata": {
        "id": "c1bJkIMcuW5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importation des données de comptage vélos depuis le fichier .csv\n",
        "bike_data = pd.read_csv('comptage-velo-donnees-compteurs.csv', sep=\";\", decimal='.').iloc[: , 1:]\n"
      ],
      "metadata": {
        "id": "7Dzx00W3ufpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant que les données ont été importées sous python, il s'agit de les comprendre. Pour commencer nous devons nous faire une idée du contenu de la base, ses dimensions, ses types de variables, etc."
      ],
      "metadata": {
        "id": "nSnqgSq5vQkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Etudes préliminaires sur la base de données**"
      ],
      "metadata": {
        "id": "9S2KVBNbvpHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Commençons par chercher le nombre de lignes et de colonnes dans cette table afin d'en étudier les dimensions. Appliquez l'attribut \"shape\" sur la dataframe pour cela. Avec pandas un attribut de dataframe 'att' est visualisable via la syntaxe 'dataframe.att'."
      ],
      "metadata": {
        "id": "g6kJCp2Bv6b4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shape = ....\n",
        "print(shape)"
      ],
      "metadata": {
        "id": "_IXd20ofwgTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shape renvoie un objet *tuple* contenant le nombre de lignes et le nombre de colonnes. Qu'observez vous ici ?\n",
        "\n",
        "Maintenant explorons de quelles variables (colonnes) nous disposons dans cette dataframe. Utilisez l'attribut \"columns\" pour cela."
      ],
      "metadata": {
        "id": "F6gtPYzxxDjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ....\n",
        "print(cols)"
      ],
      "metadata": {
        "id": "zVFg_UTtxzO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette commande nous permet d'obtenir une liste de des noms de colonnes présentes dans la dataframe. Certaines comme 'Comptage horaire', 'Date et heure de comptage' et 'Nom du compteur' semblent particulièrement intéressantes comme elles pourront situer les comptages temporellement et spatialement.\n",
        "\n",
        "Nous ne savons en revanche pas quels sont les types des colonnes (nombres entiers, continus, chaînes de caractère,etc.). Cette étape est importante, nous voulons effectivement connaitre la nature des colonnes afin de pouvoir les traiter comme il faut dans les analyses ultérieures. Vous trouverez dans la [documentation de pandas](https://pandas.pydata.org/docs/reference/frame.html) l'attribut nécessaires pour trouver le type de chaque colonne."
      ],
      "metadata": {
        "id": "chBdKBBZy07U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "types_columns = ...\n",
        "print(types_columns)"
      ],
      "metadata": {
        "id": "EF6ZR4H62I26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le type object dans pandas est un type générique qui est utilisé pour stocker des données non numériques. En pratique, il est souvent utilisé pour représenter des chaines de caractère, ou bien des valeurs hétérogènes.\n",
        "\n",
        "Nous observons notamment que la colonne 'Comptage horaire' est bien composée de comptages (int64). Les autres colonnes sont composées d'informations sous la forme de chaines de caractères, excepté la colonne 'Identifiant du site de comptage' qui est composée de valeurs numériques continues.\n",
        "\n",
        "La fonction 'describe()' peut être appliquée sur une colonne du dataframe pour en décrire le contenu (nombre de valeurs, moyenne, min, max, etc.). Appliquez cette fonction à la colonne 'Comptage horaire' afin d'en étudier le contenu. Quelles sont les valeurs de moyenne ? De médiane ? Pourquoi sont elles différentes ?"
      ],
      "metadata": {
        "id": "gJQxikRl3eZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data['Comptage horaire'].describe()"
      ],
      "metadata": {
        "id": "1GQVye1u4WBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Appliquez maintenant la fonction de description à la colonne 'Identifiant du site de comptage'. Qu'observez vous ? Est ce cohérent pour une colonne dont la donnée est censée donner un identifiant à chaque site de comptage ?\n",
        "\n"
      ],
      "metadata": {
        "id": "MJkpdCBx5b2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data['Identifiant du site de comptage'].describe()\n"
      ],
      "metadata": {
        "id": "TE3qsuba5zTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifiez la colonne 'Identifiant du site de comptage' afin d'en obtenir une variable catégorielle (chaque identifiant = une catégorie). Appliquez pour cela la fonction 'astype(...)' sur la colonne, et spécifiez que la transformation doit se faire vers des données catégorielles. Créez la colonne 'Identifiant du site de comptage cat' avec votre transformation. Ensuite appliquez la fonction de description à votre colonne modifiée 'Identifiant du site de comptage cat'. Qu'observez vous ? Combien y a t-il de sites de comptages ?"
      ],
      "metadata": {
        "id": "t58HgtgJ62eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data['Identifiant du site de comptage cat'] = ...\n",
        "bike_data['Identifiant du site de comptage cat'].describe()"
      ],
      "metadata": {
        "id": "vrH4SQcW66nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un site de comptage est une zone qui peut rassembler plusieurs capteurs de comptages répartis dans la ville de Paris. Les capteurs apparaissent dans la colonne 'Nom du compteur'. Combien y a t-il de capteurs en tout ? Vous pouvez passer par l'utilisation de la fonction 'nunique()'."
      ],
      "metadata": {
        "id": "8Hool1N_8jmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nombre_capteurs = ...\n",
        "print(nombre_capteurs)"
      ],
      "metadata": {
        "id": "rGYFIV4k8yda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A ce stade, nous ne nous sommes pas intéressé à l'aspect temporel dans les données, pourtant central pour l'étude de comptages de vélos dans le temps. La colonne 'Date et heure de comptage' est composée de chaines de caractères, visualisons l'apparence de ces données."
      ],
      "metadata": {
        "id": "C-5LPD1d9qRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data['Date et heure de comptage'].head()"
      ],
      "metadata": {
        "id": "BcjvB82Gic90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les dates et les heures sont combinées dans des chaines au pattern suivant 'AAA-MM-JJ HH:MM:SS+02:00'. Le code suivant permet d'extraire les dates et les heures : il convertit les chaines de caractères en objets datetime, desquels il est possible d'extraire les différentes composantes de temps. Deux nouvelles colonnes 'Date' et 'Heure' sont créees. Nous les convertissons en chaines de caractère pour simplifier certaines opérations ultérieures."
      ],
      "metadata": {
        "id": "mfGwaBLhjdsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Conversion des chaines de caractère en objets datetime\n",
        "bike_data['Date et heure de comptage'] = pd.to_datetime(bike_data['Date et heure de comptage'])\n",
        "\n",
        "#Extraction date\n",
        "bike_data['Date'] = bike_data['Date et heure de comptage'].dt.date.astype(str)\n",
        "\n",
        "# Extraire heure\n",
        "bike_data['Heure'] = bike_data['Date et heure de comptage'].dt.time.astype(str)"
      ],
      "metadata": {
        "id": "xXemyCFCi7QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passons maintenant à une autre étape dans l'étude de nos données : la qualité. Cette étape consiste à repérer la présence de données manquantes et d'éventuelles valeurs aberrantes."
      ],
      "metadata": {
        "id": "Sqh12iqImiOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Qualité des données**\n",
        "\n",
        "Nous allons nous concentrer dans cette partie sur les données de comptages en elles mêmes. Afin de vérifier la quantité de valeurs manquantes, il faut estimer le nombre de pas de temps, pour lesquels il n'y a pas de comptages et cela pour chaque capteur. Pour cela, dans un premier temps nous allons étendre la base de données. Nous allons créer une nouvelle base 'bike_data_spread'. Dans cette base, chaque 'Nom du compteur' de bike_data devient une colonne et les valeurs dans la colonne 'Comptage horaire' remplissent ces colonnes, [vous pouvez utiliser la méthode .'pivot()' de pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html?utm_source=chatgpt.com). Pensez à prendre comme index les colonnes 'Date' ET 'Heure'."
      ],
      "metadata": {
        "id": "m5Ilk5dUnKCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data_spread = ...\n",
        "bike_data_spread.head()"
      ],
      "metadata": {
        "id": "DhPOc4G5rP21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Votre nouvelle table devrait contenir 97 colonnes, soit le nombre de capteurs, ainsi que deux index : la date et l'heure. Nous aimerions maintenant connaitre les 5 capteurs présentant le plus de valeurs manquantes. Pour cela nous allons appliquer une série de fonctions sur la base bike_data_spread. Utilisez les fonctions isna(), sum(), sort_values() et head() pour trouver ce résultat. sort_values() utilise un paramètre 'ascending' qui vous sera utile. Ces résultats sont crées dans la base 'capteurs_valeurs_manquantes'.\n",
        "\n",
        "Aide :\n",
        "Avec pandas, il est possible d'appliquer plusieurs fonctions successives à une dataframe. Par défaut ces fonctions s'appliquent aux colonnes. Si l'on souhaite appliquer deux fonctions successives à une dataframe, on peut l'écrire sous la *dataframe.fonction1().fonction2()* où fonction2() applique une fonction à une dataframe pré-transformée par fonction1(). Par exemple bike_data_spread.mean().head(10) calculerait la moyenne des comptages pour chaque capteur, et en afficherait 10.\n",
        "\n"
      ],
      "metadata": {
        "id": "e-ARB7dPuFtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capteurs_valeurs_manquantes = ....\n",
        "print(capteurs_valeurs_manquantes)"
      ],
      "metadata": {
        "id": "dQ4ssV7wubnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant que fait on de ces données manquantes ? Ici il n'y a pas de solution toute faite, cela dépend du cas d'étude et de la quantité de données manquantes. On peut néanmoins penser à trois solutions :\n",
        "\n",
        "\n",
        "*   Dans le cas où un capteur présente une trop grande quantité de données manquantes, il devient difficile à exploiter et devrait être retiré.\n",
        "*   Dans le cas où les données manquantes sont peu nombreuses, on peut soit choisir de les ignorer, soit de les corriger via des méthodes de lissage.\n",
        "\n",
        "Dans notre cas, le capteur avec le plus de données manquantes, n'excède pas 280 manques, pour un total de 1583 pas de temps. Dans le cadre de ce TP nous conserverons les données manquantes telles quelles car elles ne présentent pas un impact important pour nos études.\n",
        "\n",
        "Passons à un rapide examen de la présence ou non de valeurs extrêmes (outliers) dans nos données. Ces données présentent des valeurs très grandes ou très petites par rapport au reste des données et peuvent soit provenir de phénomènes explicables, soit d'erreurs d'acquisitions.\n",
        "Dans le cadre d'un modèle d'apprentissage, ces données doivent être traitées, quelque soit leur origine. Si elles sont explicables, des éléments explicatifs devraient intégrer le modèle. Si elles sont le fruit d'erreurs, il faudrait les retirer ou les corriger. Une bonne manière d'étudier la distribution des valeurs dans une colonne de données, est de passer par un histogramme. Utilisez la fonction 'hist()' pour observer la distribution des données de comptages de la base 'bike_data'. [L'argument 'bins' peut vous être utile](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html).\n",
        "Que représentent les abscisses ? Les ordonnées ?\n",
        "D'après vous, les données présentent elles des valeurs extrèmes, inutilisables en tant que telles ?\n"
      ],
      "metadata": {
        "id": "_sBybcxUy6Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histo = ....\n",
        "histo"
      ],
      "metadata": {
        "id": "VlV0TP1vyuiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calcul des moments**\n",
        "\n",
        "Afin de compléter notre analyse sur la base de données, nous pouvons nous pencher sur le calcul des moments (moyennes, variances, etc.). Pour commencer nous allons nous intéresser à l'échelle journalière. Construisez une nouvelle base 'bike_data_aggr' qui contient trois colonnes : le nom du capteur, la date et le comptage journalier. Pour cela utilisez la fonction 'groupby' sur le dataset bike_data pour grouper par compteur et date. Vous devriez spécifier les colonnes sur lesquelles vous souhaitez effectuer la somme de la colonne 'Comptage horaire', à savoir les colonnes 'Nom du compteur' et 'Date'. Utilisez ensuite la fonction 'sum' sur ces groupes afin de sommer les comptages. Inspirez vous des [exemples dans l'aide](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html).\n",
        "\n",
        "Note : après groupby et sum, vous pouvez appliquer la fonction 'reset_index()'. Cette fonction transforme vos index 'Nom du compteur' et 'Date' en colonnes."
      ],
      "metadata": {
        "id": "viW_dKuDoEhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data_aggr = bike_data.groupby(['Nom du compteur', 'Date'])['Comptage horaire'].sum().reset_index()\n",
        "bike_data_aggr"
      ],
      "metadata": {
        "id": "AqfK9EFPoj6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons maintenant nous intéresser au calcul du nombre moyen de comptages de vélos, par jour, à chaque compteur. Utilisez encore groupby et la fonction 'mean' sur la base bike_data_aggr. Trouvez les 5 compteurs avec les plus gros volumes moyens journaliers."
      ],
      "metadata": {
        "id": "CBrPbncSrcvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data_aggr_mean_count = bike_data_aggr.groupby('Nom du compteur')['Comptage horaire'].mean().sort_values(ascending=False).head(5)\n",
        "print(bike_data_aggr_mean_count)"
      ],
      "metadata": {
        "id": "XhlgGXn-96w1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faites le même exercice mais pour calculer la médiane cette fois. Trouvez les 5 compteurs avec les plus gros volumes médians journaliers. Qu'observe t-on ? Obtient t-on les mêmes 5 compteurs qu'avant ? Pourquoi ?"
      ],
      "metadata": {
        "id": "QKJ8rkwS-IxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data_aggr_median_count = ...\n",
        "print(bike_data_aggr_median_count)"
      ],
      "metadata": {
        "id": "Y99L6Av6-PqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cherchez maintenant les 5 compteurs pour lesquels la variance d'utilisation quotidienne a été la plus forte au cours de la période d'étude. Pour cela utilisez la variance."
      ],
      "metadata": {
        "id": "vhLaa9tM-wDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data_aggr_var_count = ...\n",
        "print(bike_data_aggr_var_count)"
      ],
      "metadata": {
        "id": "QJsFVcKPAh55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trouvez le jour où la fréquentation a été la plus forte sur l'ensemble des pistes cyclables. Que remarquez vous ? Les grandes vacances avaient elles commencé ?"
      ],
      "metadata": {
        "id": "HUN2vviKBZ6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data_aggr_sum_tot = ...\n",
        "print(bike_data_aggr_sum_tot)"
      ],
      "metadata": {
        "id": "d6cwU0IdBprn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repartez de la dataframe bike_data et trouvez les 6 heures de la journée où la fréquentation médiane est la plus forte sur l'ensemble de la période. Que remarquez vous sur les heures trouvées ? Que représentent elles ?"
      ],
      "metadata": {
        "id": "tFIqz5txDLQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data_median_hour = ...\n",
        "print(bike_data_median_hour)"
      ],
      "metadata": {
        "id": "TpbcDEntDMuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant que nous avons vu les notions de moyenne, de médiane et de variance, intéressons nous à leurs étendues. A partir de bike_data, créez une base bike_data_filt qui contient les comptages à chaque jour mais uniquement sur l'heure ayant la fréquentation médiane la plus forte (que vous avez trouvé précédemment). Le filtrage se fait de la manière suivante, pour une base de données df où 'on souhaite filtrer les valeurs 'a' de la colonne 'A', le filtrage se fait comme : df[df['A'] == 'a'].\n",
        "\n",
        "Construisez ensuite une boite à moustache (boxplot) à l'aide de la fonction 'plot.box()' appliquée sur la colonne 'Comptage horaire' de votre base bike_data_filt. Quelle est la valeur approximative de 3e quartile ? Que représente t-il ?"
      ],
      "metadata": {
        "id": "vTVDDTkXEQrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bike_data_filt = ...\n",
        "bike_data_filt['Comptage horaire'].plot.box(figsize=(6, 4), grid=True)"
      ],
      "metadata": {
        "id": "fpuXWk33H2Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour finir nous allons nous intéresser à la notion de covariance cov(x1,x2). Cette métrique décrit la manière dont évolue une quantité (x1) lorsqu'une autre quantité change de valeur (x2). Nous souhaitons trouver les 5 couples de compteurs ayant les plus fortes covariances positives, puis négatives, sur la même heure que vous avez utilisé précédemment. Nous allons travailler avec la base étendue bike_data_spread, nécessaire pour calculer les covariances. Pour commencer nous transformons les index 'Date' et 'Heure' en colonnes que l'on peut manipuler."
      ],
      "metadata": {
        "id": "MKE-quJ-Ky_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer les index 'Date' et 'Heure' en colonnes\n",
        "bike_data_spread = bike_data_spread.reset_index()"
      ],
      "metadata": {
        "id": "hpFlORbCIm1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Créez ensuite une base de données 'bike_data_spread_filt' qui correspond à la base étendue, filtrer sur l'heure à la fréquentation médiane max que vous avez déterminé précédemment. Créez une matrice de covariance 'cov_mat' à partir de votre base filtrée. Utilisez pour cela la fonction 'cov()' pour déterminer les covariances entre chaque couple de compteur ([l'aide ici ](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.cov.html)). On transforme ensuite cette matrice en dataframe avec pour chaque couple 'Compteur1' et 'Compteur2' la covariance associée. On retire les couples pour lesquels 'Compteur1' == 'Compteur2'. On trie ensuite dans l'ordre décroissant ou croissant des valeurs de covariance."
      ],
      "metadata": {
        "id": "BT5daKxMOBYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtrer la base sur une heure\n",
        "bike_data_spread_filt = ...\n",
        "\n",
        "#Ramener Date et Heure à des index pour faciliter les manipulation de covariance\n",
        "bike_data_spread_filt = bike_data_spread_filt.set_index(['Date', 'Heure'])\n",
        "\n",
        "#Matrice de covariance\n",
        "cov_mat = ...\n",
        "# Masquer la partie inférieure de la matrice (i > j)\n",
        "mask = np.triu(np.ones(cov_mat.shape), k=1)\n",
        "# Appliquer le masque à la matrice de covariance\n",
        "upper_cov_matrix = cov_mat * mask\n",
        "\n",
        "#Transformation en dataframe avec deux colonnes compteur1 et compteur2.\n",
        "stacked_cov = upper_cov_matrix.stack()\n",
        "stacked_cov.index.names = ['Compteur1', 'Compteur2']\n",
        "stacked_cov = stacked_cov.reset_index()\n",
        "stacked_cov.columns = ['Compteur1', 'Compteur2','Covariance']\n",
        "\n",
        "#Trier dans l'ordre décroissant des valeurs de covariance - covariances positives, puis négatives\n",
        "stacked_cov.sort_values('Covariance', ascending=False).head(5)\n",
        "stacked_cov.sort_values('Covariance', ascending=True).head(5)"
      ],
      "metadata": {
        "id": "1IBpg_gOOQRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merci pour votre participation à ce premier atelier. Dans le prochain atelier nous nous intéresserons aux notions de clustering qui permettront de déterminer quels jours et/ou quels compteurs se ressemblent en terme de trafic de vélos."
      ],
      "metadata": {
        "id": "UG5BIoiLcD3G"
      }
    }
  ]
}